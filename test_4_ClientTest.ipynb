{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3682e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This is the same example as 3-test_EvalwithClient.py but in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de17d2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval import evaluate\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.agents.models import ListSortOrder\n",
    "from azure.identity import AzureCliCredential\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7147dad7",
   "metadata": {},
   "source": [
    "Define Azure Ai Project, get Ai Foundry Agent and run the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84cb4d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure AI Project Client\n",
    "def get_project_client():\n",
    "    endpoint = os.environ.get(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
    "    if not endpoint:\n",
    "        raise ValueError(\"AZURE_AI_PROJECT_ENDPOINT not set in environment.\")\n",
    "    return AIProjectClient(endpoint=endpoint, credential=AzureCliCredential())\n",
    "\n",
    "# Get agent by ID\n",
    "def get_agent(client, agent_id):\n",
    "    agent = client.agents.get_agent(agent_id)\n",
    "    if not agent:\n",
    "        raise ValueError(f\"Agent with ID {agent_id} not found.\")\n",
    "    return agent\n",
    "\n",
    "# Run agent thread and get response\n",
    "def run_agent_thread(client, agent_id, user_message):\n",
    "    final_run = client.agents.create_thread_and_process_run(\n",
    "        agent_id=agent_id,\n",
    "        thread={\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ]\n",
    "        },\n",
    "    )\n",
    "    if final_run.status != \"completed\":\n",
    "        raise RuntimeError(\"Agent run did not complete successfully.\")\n",
    "    messages = client.agents.messages.list(thread_id=final_run.thread_id, order=ListSortOrder.ASCENDING)\n",
    "    for message in messages:\n",
    "        if message.run_id == final_run.id and message.text_messages:\n",
    "            return message.text_messages[-1].text.value\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb1b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metric\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the facts in 'actual output' contradict any facts in 'expected output'\",\n",
    "        \"You should also heavily penalize omission of detail\",\n",
    "        \"Vague language, or contradicting OPINIONS, are OK\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9c7164",
   "metadata": {},
   "source": [
    "Get the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8406c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_project_client()\n",
    "agent_id = \"asst_IpYYAJDLI5qwEc2XcEe7CGkc\"\n",
    "user_question = \"The dog chased the cat up the tree, who ran up the tree?\"\n",
    "agent = get_agent(client, agent_id)\n",
    "response = run_agent_thread(client, agent.id, user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e817ac",
   "metadata": {},
   "source": [
    "Run the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246a6e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">[</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">]</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using Azure OpenAI </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness \u001b[0m\u001b[1;38;2;106;0;255m[\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m]\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing Azure OpenAI \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mgpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚ùå Correctness [GEval] (score: 0.0, threshold: 0.5, strict: False, evaluation model: Azure OpenAI (gpt-4.1), reason: The actual output directly contradicts the expected output by stating that the cat ran up the tree, while the expected output is 'The dog.' This is a clear factual contradiction and a critical error according to the evaluation steps., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: The dog chased the cat up the tree, who ran up the tree?\n",
      "  - actual output: The cat ran up the tree.\n",
      "  - expected output: The dog.\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Correctness [GEval]: 0.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Done üéâ! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmezc8wwy041vzt5726njpftj/evaluation/test-runs/cmfg253to03h98d181o34lxg6/compare-test-results\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmezc8wwy041vzt5726njpftj/evaluation/test-runs/cmfg253to03h98d181o34lxg6/compa</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmezc8wwy041vzt5726njpftj/evaluation/test-runs/cmfg253to03h98d181o34lxg6/compare-test-results\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">re-test-results</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Done üéâ! View results on \n",
       "\u001b]8;id=675377;https://app.confident-ai.com/project/cmezc8wwy041vzt5726njpftj/evaluation/test-runs/cmfg253to03h98d181o34lxg6/compare-test-results\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmezc8wwy041vzt5726njpftj/evaluation/test-runs/cmfg253to03h98d181o34lxg6/compa\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=675377;https://app.confident-ai.com/project/cmezc8wwy041vzt5726njpftj/evaluation/test-runs/cmfg253to03h98d181o34lxg6/compare-test-results\u001b\\\u001b[4;94mre-test-results\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=False, metrics_data=[MetricData(name='Correctness [GEval]', threshold=0.5, success=False, score=0.0, reason=\"The actual output directly contradicts the expected output by stating that the cat ran up the tree, while the expected output is 'The dog.' This is a clear factual contradiction and a critical error according to the evaluation steps.\", strict_mode=False, evaluation_model='Azure OpenAI (gpt-4.1)', error=None, evaluation_cost=0.001088, verbose_logs='Criteria:\\nDetermine whether the actual output is factually correct based on the expected output. \\n \\nEvaluation Steps:\\n[\\n    \"Check whether the facts in \\'actual output\\' contradict any facts in \\'expected output\\'\",\\n    \"You should also heavily penalize omission of detail\",\\n    \"Vague language, or contradicting OPINIONS, are OK\"\\n] \\n \\nRubric:\\nNone \\n \\nScore: 0.0')], conversational=False, multimodal=False, input='The dog chased the cat up the tree, who ran up the tree?', actual_output='The cat ran up the tree.', expected_output='The dog.', context=None, retrieval_context=None, additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cmezc8wwy041vzt5726njpftj/evaluation/test-runs/cmfg253to03h98d181o34lxg6/compare-test-results')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case = LLMTestCase(\n",
    "        input=user_question,\n",
    "        actual_output=response,\n",
    "        expected_output=\"The dog.\"\n",
    "    )\n",
    "\n",
    "evaluate(test_cases=[test_case], metrics=[correctness_metric])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
