{"test_cases_lookup_map": {"{\"actual_output\": \"It depends, some might consider the cat, while others might argue the dog.\", \"context\": null, \"expected_output\": \"The cat.\", \"hyperparameters\": null, \"input\": \"The dog chased the cat up the tree, who ran up the tree?\", \"retrieval_context\": null}": {"cached_metrics_data": [{"metric_data": {"name": "Correctness [GEval]", "threshold": 0.5, "success": false, "score": 0.19890130594035196, "reason": "The actual output does not contradict the expected output but omits the key detail that the cat ran up the tree, which is explicitly stated in the expected output. The response is vague and fails to provide the specific answer required, resulting in a significant penalty for omission of detail.", "strictMode": false, "evaluationModel": "Azure OpenAI (gpt-4.1)", "evaluationCost": 0, "verboseLogs": "Criteria:\nDetermine whether the actual output is factually correct based on the expected output. \n \nEvaluation Steps:\n[\n    \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n    \"You should also heavily penalize omission of detail\",\n    \"Vague language, or contradicting OPINIONS, are OK\"\n] \n \nRubric:\nNone \n \nScore: 0.19890130594035196"}, "metric_configuration": {"threshold": 0.5, "evaluation_model": "Azure OpenAI (gpt-4.1)", "strict_mode": false, "criteria": "Determine whether the actual output is factually correct based on the expected output.", "include_reason": false, "evaluation_steps": ["Check whether the facts in 'actual output' contradicts any facts in 'expected output'", "You should also heavily penalize omission of detail", "Vague language, or contradicting OPINIONS, are OK"], "evaluation_params": ["input", "actual_output", "expected_output"]}}]}}}